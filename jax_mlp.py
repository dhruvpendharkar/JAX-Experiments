# -*- coding: utf-8 -*-
"""JAX MLP

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xFrfxmtcFbW-V0nq7tmLkIlu4NPNQ7Ps
"""

import jax
import jax.numpy as jnp
import jax.random as random
from jax.scipy.special import logsumexp
from jax.nn import relu
from functools import partial
from jax.tree_util import register_pytree_node_class

@register_pytree_node_class
class JAXMLP():
  def __init__(self, parameters, layers):
    self.parameters = parameters
    self.layers = layers

    if (not self.parameters) and self.layers:
      self.random_init_params()

  def random_init_params(self):
    initializer = jax.nn.initializers.normal(0.01)
    self.parameters = [
            [initializer(jax.random.PRNGKey(55), (n, m), dtype=jnp.float32),
             initializer(jax.random.PRNGKey(55), (n,), dtype=jnp.float32)]
            for m, n in zip(self.layer_sizes[:-1], self.layer_sizes[1:])]

  @partial(jax.vmap, in_axes=(None, None, 0))
  def forward(self, parameters, x):
    init = x
    for w, b in parameters[:-1]:
      out = jnp.dot(w, init) + b
      init = jax.nn.relu(out)
    w, b = parameters[-1]
    out = jnp.dot(w, init) + b
    return out - logsumexp(out)

  def predict(self, x):
    return jnp.argmax(self.forward(self.parameters, x), axis=1)

  @jax.jit
  def loss(self, params, x, y):
    y_pred = self.forward(params, x)
    return -jnp.mean(y_pred * y)

  @jax.jit
  def backward(self, x, y, lr):
    grads = jax.grad(self.loss)(self.parameters, x, y)
    new_params = [[w - lr * dw, b - lr*db] for (w, b), (dw, db) in zip(self.parameters, grads)]
    return new_params

  def train(self, x, y, epochs, lr=0.01):
    for epoch in range(epochs):
      print(f'Epoch {epoch}')
      loss = self.loss(self.parameters, x, y)
      print(f'Loss: {loss}')
      self.parameters = self.backward(x, y, lr)
    print("Training Complete")
    final_loss = self.loss(self.parameters, x, y)
    print(f'Final Loss: {final_loss}')

  def tree_flatten(self):
      children = (self.parameters, self.layer_sizes)  # arrays / dynamic values
      aux_data = {}  # static values
      return (children, aux_data)

  @classmethod
  def tree_unflatten(cls, aux_data, children):
      return cls(*children, **aux_data)